{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 0, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 0, 1, 1, 0, 0, 0, 0]])\n",
      "tensor([[1, 0, 1, 1, 0],\n",
      "        [1, 0, 1, 0, 0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkj/ProtDiffusion/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tensor = torch.tensor([[1,1,1,0,1,1,1,1,0,0],[1,1,1,0,1,1,0,0,0,0]], dtype=torch.int64)\n",
    "print(tensor)\n",
    "\n",
    "tensor = -tensor.to(torch.float32)\n",
    "tensor = F.max_pool1d(tensor, kernel_size=2, stride=2, padding=0)\n",
    "tensor = -tensor.to(torch.int64)\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('/home/kkj/ProtDiffusion/ProtDiffusion/tokenizer/tokenizer_v4.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2, 23,  3,  4,  5,  7,  8,  5, 10,  8,  5,  6, 24,  2,  2,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer('-[ACDFGDIGDE]---',\n",
    "                        padding=True,\n",
    "                        truncation=False, # We truncate the sequences beforehand\n",
    "                        return_token_type_ids=False,\n",
    "                        return_attention_mask=True, # We need to attend to padding tokens, so we set this to False\n",
    "                        return_tensors=\"pt\",\n",
    ")\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(tokenized['attention_mask'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.randint(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def round_length(length: int, pad: int = 2, rounding: int = 16) -> int:\n",
    "    '''\n",
    "    Round the length to the nearest multiple of 16.\n",
    "    '''\n",
    "    return int(np.ceil((length + pad) / rounding) * rounding)\n",
    "\n",
    "def process_sequence(sequence: str,\n",
    "                     bos_token: str = \"[\",\n",
    "                     eos_token: str = \"]\",\n",
    "                     pad_token: str = \"-\",\n",
    ") -> str:\n",
    "    '''\n",
    "    Process the sequence by adding the bos and eos tokens, and padding it to a multiple of 16 (or what the variable is set to in the round_kength).\n",
    "    Return the sequence and the length of the sequence.\n",
    "    '''\n",
    "    seq_len = round_length(len(sequence))\n",
    "    sequence = bos_token + sequence + eos_token\n",
    "    len_diff = seq_len - len(sequence)\n",
    "    rand_int = random.randint(0, len_diff)\n",
    "    sequence = pad_token * rand_int + sequence + pad_token * (len_diff - rand_int)\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ACDFGDIGDEIJGH]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_sequence('ACDFGDIGDEIJGH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Sinusoidal Positional Embeddings\n",
      "num_positional_embeddings:  1024\n",
      "Using RoPE\n",
      "RoPE dim:  72\n",
      "Model has 207216064 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DiTTransformer1DModel(\n",
       "  (conv_in): Conv1d(64, 1152, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (rotary_emb): RotaryEmbedding()\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-7): 8 x BasicTransformerBlock1D(\n",
       "      (pos_embed): SinusoidalPositionalEmbedding()\n",
       "      (norm1): AdaLayerNormZero(\n",
       "        (emb): CombinedTimestepLabelEmbeddings(\n",
       "          (time_proj): Timesteps()\n",
       "          (timestep_embedder): TimestepEmbedding(\n",
       "            (linear_1): Linear(in_features=256, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (linear_2): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (class_embedder): LabelEmbedding(\n",
       "            (embedding_table): Embedding(3, 1152)\n",
       "          )\n",
       "        )\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=1152, out_features=6912, bias=True)\n",
       "        (norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "      (attn1): Attention(\n",
       "        (to_q): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "        (to_k): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "        (to_v): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((1152,), eps=1e-05, elementwise_affine=False)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=1152, out_features=4608, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=4608, out_features=1152, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_out): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
       "  (proj_out_1): Linear(in_features=1152, out_features=2304, bias=True)\n",
       "  (proj_out_2): Linear(in_features=1152, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.dit_transformer_1d import DiTTransformer1DModel\n",
    "from training_utils import count_parameters\n",
    "\n",
    "model = DiTTransformer1DModel(\n",
    "    num_attention_heads = 16,\n",
    "    attention_head_dim = 72,\n",
    "    in_channels = 64,\n",
    "    num_layers = 8,\n",
    "    attention_bias = True,\n",
    "    activation_fn = \"gelu-approximate\",\n",
    "    num_classes = 2,\n",
    "    upcast_attention = False,\n",
    "    norm_type = \"ada_norm_zero\",\n",
    "    norm_elementwise_affine = False,\n",
    "    norm_eps = 1e-5,\n",
    "    pos_embed_type = \"sinusoidal\", # sinusoidal\n",
    "    num_positional_embeddings = 1024,\n",
    "    use_rope_embed = True, # RoPE https://github.com/lucidrains/rotary-embedding-torch\n",
    ").to('cuda')\n",
    "count_parameters(model)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 1024])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(16, 64, 1024).to('cuda')\n",
    "m = torch.randint(0, 2, (16, 1024), dtype=torch.bool).to('cuda')\n",
    "t = torch.randint(0, 1000, (16,), dtype=torch.int64).to('cuda') # Timesteps, any int is valid?\n",
    "cl = torch.randint(0, 3, (16,), dtype=torch.int64).to('cuda') # Classifier labels, 0 and 1 are the only valid labels, 2 is a dropped label\n",
    "\n",
    "out = model(x, m, t, cl)\n",
    "print(out.sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
