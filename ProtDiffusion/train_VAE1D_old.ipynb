{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'mega_batch': 1000, 'num_epochs': 1, 'gradient_accumulation_steps': 2, 'learning_rate': 0.0001, 'lr_warmup_steps': 1000, 'save_image_model_steps': 100, 'mixed_precision': 'fp16', 'optimizer': 'AdamW', 'SGDmomentum': 0.9, 'output_dir': 'output/protein-VAE-UniRef50-7', 'pad_to_multiple_of': 16, 'max_len': 512, 'seed': 42, 'automatic_checkpoint_naming': True, 'total_limit': 1, 'cutoff': None, 'kl_weight': 0.05, 'weight_decay': 0.01, 'grokfast': False, 'grokfast_alpha': 0.98, 'grokfast_lamb': 2.0}\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "from typing import Optional, Literal, Union, List\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    batch_size: int = 64  # the batch size\n",
    "    mega_batch: int = 1000 # how many batches to use for batchsampling\n",
    "    num_epochs: int = 1  # the number of epochs to train the model\n",
    "    gradient_accumulation_steps: int = 2  # the number of steps to accumulate gradients before taking an optimizer step\n",
    "    learning_rate: float = 1e-4  # the learning rate\n",
    "    lr_warmup_steps:int  = 1000\n",
    "    save_image_model_steps:int  = 100\n",
    "    mixed_precision: str = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    optimizer: str = \"AdamW\"  # the optimizer to use, choose between `AdamW`, `Adam`, `SGD`, and `Adamax`\n",
    "    SGDmomentum: float = 0.9\n",
    "    output_dir: str = os.path.join(\"output\",\"protein-VAE-UniRef50-8\")  # the model name locally and on the HF Hub\n",
    "    pad_to_multiple_of: int = 16\n",
    "    max_len: int = 512  # truncation of the input sequence\n",
    "\n",
    "    class_embeddings_concat = False  # whether to concatenate the class embeddings to the time embeddings\n",
    "\n",
    "    push_to_hub = False  # Not implemented yet. Whether to upload the saved model to the HF Hub\n",
    "    hub_model_id = \"kkj15dk/protein-VAE\"  # the name of the repository to create on the HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed: int = 42\n",
    "\n",
    "    automatic_checkpoint_naming: bool = True  # whether to automatically name the checkpoints\n",
    "    total_limit: int = 1  # the total limit of checkpoints to save\n",
    "\n",
    "    labels_file = 'labels_test.json'\n",
    "\n",
    "    cutoff: Optional[float] = None # cutoff for when to predict the token given the logits, and when to assign the unknown token 'X' to this position\n",
    "    skip_special_tokens = False # whether to skip the special tokens when writing the evaluation sequences\n",
    "    kl_weight: float = 0.05 # the weight of the KL divergence in the loss function\n",
    "\n",
    "    weight_decay: float = 0.01 # weight decay for the optimizer\n",
    "    grokfast: bool = False # whether to use the grokfast algorithm\n",
    "    grokfast_alpha: float = 0.98 #Momentum hyperparmeter of the EMA.\n",
    "    grokfast_lamb: float = 2.0 #Amplifying factor hyperparameter of the filter.\n",
    "\n",
    "config = TrainingConfig()\n",
    "print(vars(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# # config.dataset_name = \"kkj15dk/test_dataset\"\n",
    "# config.dataset_name = \"agemagician/uniref50\"\n",
    "# # config.dataset_name = \"kkj15dk/UniRef50-encoded\"\n",
    "# dataset = load_dataset(config.dataset_name) # , download_mode='force_redownload')\n",
    "# dataset = dataset.shuffle(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"kkj15dk/protein_tokenizer\")\n",
    "\n",
    "def encode(example):\n",
    "    return tokenizer(example['text'],\n",
    "    # return tokenizer(example['sequence'],\n",
    "                    padding = True,\n",
    "                    pad_to_multiple_of = config.pad_to_multiple_of,\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=False, # We need to attend to padding tokens, so we set this to False\n",
    ")\n",
    "# dataset_train = dataset['train'].map(encode, batched=False, remove_columns=[\"sequence\"])\n",
    "# dataset_test = dataset['test'].map(encode, batched=False, remove_columns=[\"sequence\"])\n",
    "# dataset_val = dataset['val'].map(encode, batched=False, remove_columns=[\"sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train_dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b9da9a1a8546aeb285e86dd1829574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading test_dataset\n",
      "loading val_dataset\n"
     ]
    }
   ],
   "source": [
    "# dataset_train = dataset['train']\n",
    "# dataset_test = dataset['test']\n",
    "# dataset_val = dataset['val']\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "datasets_folder = 'datasets'\n",
    "\n",
    "train_file = os.path.join(datasets_folder, 'train_dataset')\n",
    "test_file = os.path.join(datasets_folder, 'test_dataset')\n",
    "val_file = os.path.join(datasets_folder, 'val_dataset')\n",
    "\n",
    "if os.path.exists(train_file):\n",
    "    print('loading train_dataset')\n",
    "    dataset_train = load_from_disk(train_file)\n",
    "else:\n",
    "    print('encoding train_dataset')\n",
    "    dataset_train = dataset['train'].map(encode, batched=False, remove_columns=[\"text\"], num_proc=os.cpu_count())\n",
    "    dataset_train.save_to_disk(train_file)\n",
    "\n",
    "if os.path.exists(test_file):\n",
    "    print('loading test_dataset')\n",
    "    dataset_test = load_from_disk(test_file)\n",
    "else:\n",
    "    print('encoding test_dataset')\n",
    "    dataset_test = dataset['test'].map(encode, batched=False, remove_columns=[\"text\"], num_proc=os.cpu_count())\n",
    "    dataset_test.save_to_disk(test_file)\n",
    "\n",
    "if os.path.exists(val_file):\n",
    "    print('loading val_dataset')\n",
    "    dataset_val = load_from_disk(val_file)\n",
    "else:\n",
    "    print('encoding val_dataset')\n",
    "    dataset_val = dataset['validation'].map(encode, batched=False, remove_columns=[\"text\"], num_proc=os.cpu_count())\n",
    "    dataset_val.save_to_disk(val_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'name', 'input_ids'],\n",
      "    num_rows: 6084\n",
      "})\n",
      "{'id': 4909, 'name': 'UniRef50_E0S3A2', 'input_ids': [0, 13, 18, 12, 14, 6, 12, 8, 11, 5, 18, 16, 3, 10, 20, 6, 11, 12, 19, 8, 5, 6, 17, 7, 13, 18, 17, 10, 19, 18, 10, 8, 12, 19, 15, 8, 4, 18, 10, 17, 20, 10, 17, 14, 5, 11, 14, 17, 15, 13, 12, 10, 22, 18, 17, 5, 19, 10, 10, 3, 12, 14, 17, 14, 6, 4, 11, 8, 10, 6, 20, 3, 6, 20, 3, 8, 1, 23, 23, 23]}\n",
      "[0, 13, 18, 12, 14, 6, 12, 8, 11, 5, 18, 16, 3, 10, 20, 6, 11, 12, 19, 8, 5, 6, 17, 7, 13, 18, 17, 10, 19, 18, 10, 8, 12, 19, 15, 8, 4, 18, 10, 17, 20, 10, 17, 14, 5, 11, 14, 17, 15, 13, 12, 10, 22, 18, 17, 5, 19, 10, 10, 3, 12, 14, 17, 14, 6, 4, 11, 8, 10, 6, 20, 3, 6, 20, 3, 8, 1, 23, 23, 23]\n",
      "UniRef50_E0S3A2\n",
      "80\n",
      "[MSLNELGKDSQAIVEKLTGDERFMSRITSIGLTPGCSIRVIRNDKNRPMLIYSRDTIIALNRNECKGIEVAEVAG]---\n",
      "MSLNELGKDSQAIVEKLTGDERFMSRITSIGLTPGCSIRVIRNDKNRPMLIYSRDTIIALNRNECKGIEVAEVAG\n"
     ]
    }
   ],
   "source": [
    "print(dataset_val)\n",
    "print(dataset_val[0])\n",
    "print(dataset_val[0]['input_ids'])\n",
    "print(dataset_val[0]['name'])\n",
    "print(len(dataset_val[0]['input_ids']))\n",
    "print(tokenizer.decode(dataset_val[0]['input_ids'], skip_special_tokens=False))\n",
    "print(tokenizer.decode(dataset_val[0]['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 80])\n",
      "tensor([[ 0, 13, 18, 12, 14,  6, 12,  8, 11,  5, 18, 16,  3, 10, 20,  6, 11, 12,\n",
      "         19,  8,  5,  6, 17,  7, 13, 18, 17, 10, 19, 18, 10,  8, 12, 19, 15,  8,\n",
      "          4, 18, 10, 17, 20, 10, 17, 14,  5, 11, 14, 17, 15, 13, 12, 10, 22, 18,\n",
      "         17,  5, 19, 10, 10,  3, 12, 14, 17, 14,  6,  4, 11,  8, 10,  6, 20,  3,\n",
      "          6, 20,  3,  8,  1, 23, 23, 23]])\n",
      "['[MSLNELGKDSQAIVEKLTGDERFMSRITSIGLTPGCSIRVIRNDKNRPMLIYSRDTIIALNRNECKGIEVAEVAG]---']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids_tensor = torch.tensor(dataset_val[0]['input_ids'], dtype=torch.long)\n",
    "onehot_seq = F.one_hot(input_ids_tensor, num_classes=tokenizer.vocab_size + 1).permute(1, 0).unsqueeze(0)\n",
    "print(onehot_seq.shape)\n",
    "\n",
    "def logits_to_token_ids(tokenizer, logits, cutoff = None):\n",
    "        '''\n",
    "        Convert a batch of logits to token_ids.\n",
    "        Returns token_ids\n",
    "        '''\n",
    "        if cutoff is None:\n",
    "            token_ids = logits.argmax(dim=1)\n",
    "        else:\n",
    "            token_ids = torch.where(logits.max(dim=1).values > cutoff, \n",
    "                                    logits.argmax(dim=1), \n",
    "                                    torch.tensor([tokenizer.unknown_token_id])\n",
    "                                    )\n",
    "\n",
    "        return token_ids\n",
    "token_ids = logits_to_token_ids(tokenizer, onehot_seq)\n",
    "print(token_ids)\n",
    "print(tokenizer.batch_decode(token_ids, skip_special_tokens=config.skip_special_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def collate_fn(batch): # Can definitely be optimized\n",
    "    max_len = max(len(x['input_ids']) for x in batch)\n",
    "    if max_len > config.max_len:\n",
    "        max_len = config.max_len\n",
    "    input_ids = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    attention_mask = torch.zeros(len(batch), max_len, dtype=torch.float)\n",
    "    class_labels = torch.zeros(len(batch), dtype=torch.long)\n",
    "    # identifiers = [x['id'] for x in batch]\n",
    "    identifiers = [x.get('name', 'N/A') for x in batch]\n",
    "    for i, x in enumerate(batch):\n",
    "        seq_len = len(x['input_ids'])\n",
    "        if seq_len > max_len:\n",
    "            index = random.randint(0, seq_len - max_len)\n",
    "            x['input_ids'] = x['input_ids'][index:index+max_len]\n",
    "            seq_len = max_len\n",
    "        input_ids[i, :seq_len] = torch.tensor(x['input_ids'], dtype=torch.long)\n",
    "        attention_mask[i, :seq_len] = torch.tensor(1, dtype=torch.float)\n",
    "        # class_labels[i] = torch.tensor(x['class'], dtype=torch.long)\n",
    "    return {'id': identifiers, 'input_ids': input_ids, 'attention_mask': attention_mask, 'class_label': class_labels}\n",
    "\n",
    "class BatchSampler:\n",
    "    '''\n",
    "    BatchSampler for variable length sequences, batching by similar lengths, to prevent excessive padding.\n",
    "    '''\n",
    "    def __init__(self, lengths, batch_size, mega_batch_size, drop_last = True):\n",
    "        self.lengths = lengths\n",
    "        self.batch_size = batch_size\n",
    "        self.mega_batch_size = mega_batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self):\n",
    "        size = len(self.lengths)\n",
    "        indices = list(range(size))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        step = self.mega_batch_size * self.batch_size\n",
    "        for i in range(0, size, step):\n",
    "            pool = indices[i:i+step]\n",
    "            pool = sorted(pool, key=lambda x: self.lengths[x])\n",
    "            mega_batch_indices = list(range(0, len(pool), self.batch_size))\n",
    "            random.shuffle(mega_batch_indices) # shuffle the mega batches, so that the model doesn't see the same order of lengths every time. The small batch will however always be the one with longest lengths\n",
    "            for j in mega_batch_indices:\n",
    "                if self.drop_last and j + self.batch_size > len(pool): # drop the last batch if it's too small\n",
    "                    continue\n",
    "                batch = pool[j:j+self.batch_size]\n",
    "                random.shuffle(batch) # shuffle the batch, so that the model doesn't see the same order of lengths every time\n",
    "                yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.lengths) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.lengths) + self.batch_size - 1) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_lengths_path = os.path.join(datasets_folder, 'train_lengths.pkl')\n",
    "val_lengths_path = os.path.join(datasets_folder, 'val_lengths.pkl')\n",
    "test_lengths_path = os.path.join(datasets_folder, 'test_lengths.pkl')\n",
    "\n",
    "if os.path.exists(train_lengths_path):\n",
    "    with open(train_lengths_path, 'rb') as f:\n",
    "        train_lengths = pickle.load(f)\n",
    "else:\n",
    "    train_lengths = list(map(lambda x: len(x[\"input_ids\"]), dataset_train))\n",
    "    with open(train_lengths_path, 'wb') as f:\n",
    "        pickle.dump(train_lengths, f)\n",
    "\n",
    "if os.path.exists(val_lengths_path):\n",
    "    with open(val_lengths_path, 'rb') as f:\n",
    "        val_lengths = pickle.load(f)\n",
    "else:\n",
    "    val_lengths = list(map(lambda x: len(x[\"input_ids\"]), dataset_val))\n",
    "    with open(val_lengths_path, 'wb') as f:\n",
    "        pickle.dump(val_lengths, f)\n",
    "\n",
    "if os.path.exists(test_lengths_path):\n",
    "    with open(test_lengths_path, 'rb') as f:\n",
    "        test_lengths = pickle.load(f)\n",
    "else:\n",
    "    test_lengths = list(map(lambda x: len(x[\"input_ids\"]), dataset_test))\n",
    "    with open(test_lengths_path, 'wb') as f:\n",
    "        pickle.dump(test_lengths, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataloader = DataLoader(dataset_test,\n",
    "                            batch_sampler=BatchSampler(test_lengths,\n",
    "                                                    config.batch_size,\n",
    "                                                    config.mega_batch,\n",
    "                                                    drop_last=False), \n",
    "                            collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(dataset_val, \n",
    "                            batch_sampler=BatchSampler(val_lengths,\n",
    "                                                    config.batch_size,\n",
    "                                                    config.mega_batch,\n",
    "                                                    drop_last=False),\n",
    "                            collate_fn=collate_fn)\n",
    "train_dataloader = DataLoader(dataset_train,\n",
    "                            batch_sampler=BatchSampler(train_lengths,\n",
    "                                                    config.batch_size,\n",
    "                                                    config.mega_batch,\n",
    "                                                    drop_last=False),\n",
    "                            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from New1D.autoencoder_kl_1d import AutoencoderKL1D\n",
    "\n",
    "model = AutoencoderKL1D(\n",
    "    num_class_embeds=tokenizer.vocab_size + 1,  # the number of class embeddings\n",
    "    \n",
    "    down_block_types=(\n",
    "        \"DownEncoderBlock1D\",  # a regular ResNet downsampling block\n",
    "        \"DownEncoderBlock1D\",\n",
    "        \"DownEncoderBlock1D\",\n",
    "        \"DownEncoderBlock1D\",  # a ResNet downsampling block with spatial self-attention\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpDecoderBlock1D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpDecoderBlock1D\",\n",
    "        \"UpDecoderBlock1D\",\n",
    "        \"UpDecoderBlock1D\",  # a regular ResNet upsampling block\n",
    "    ),\n",
    "    block_out_channels=(128, 256, 512, 512),  # the number of output channels for each block\n",
    "    mid_block_type=\"UNetMidBlock1D\",  # the type of the middle block\n",
    "    mid_block_channels=1024,  # the number of output channels for the middle block\n",
    "    mid_block_add_attention=False,  # whether to add a spatial self-attention block to the middle block\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    transformer_layers_per_block=1, # how many transformer layers to use per ResNet layer. Not implemented yet.\n",
    "\n",
    "    latent_channels=128,  # the dimensionality of the latent space\n",
    "\n",
    "    num_attention_heads=1,  # the number of attention heads in the spatial self-attention blocks\n",
    "    upsample_type=\"conv\", # the type of upsampling to use, either 'conv' (and nearest neighbor) or 'conv_transpose'\n",
    "    act_fn=\"gelu\",  # the activation function to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61429144"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.children\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'input_ids', 'attention_mask', 'class_label'])\n",
      "UniRef50_A0A1F4Y1L9\n",
      "tensor([18, 15, 12, 16, 14,  7,  3, 20, 18, 16, 20, 10, 14, 14, 12, 18,  5, 20,\n",
      "        17, 12, 14,  8,  5, 19, 14,  3,  5, 14, 10,  7,  3,  5, 12, 20,  9, 19,\n",
      "         8, 17, 12,  5, 20, 19,  8,  5, 13, 14, 20, 18,  8,  3,  7, 14,  3,  3,\n",
      "         8,  8, 20,  7, 19, 14, 12, 19, 20, 14,  8, 14,  3, 19, 19, 19,  8, 14,\n",
      "        18, 10, 20, 12,  8,  5, 12, 18, 19, 16,  8,  8, 10,  7,  8, 18, 18, 19,\n",
      "        12, 19, 10, 12,  8,  5, 19, 19, 12,  3, 14,  3, 19, 18, 19, 14,  7,  7,\n",
      "        18, 19, 19,  3, 18, 18, 19, 14, 12,  7,  3, 16, 18,  8, 18, 10,  8, 19,\n",
      "        12, 18,  3, 16, 19, 12, 14, 12, 18,  8, 12,  3, 18,  7, 12, 14,  8,  7,\n",
      "        19, 18, 12,  3, 18, 18, 19, 10,  8,  5,  8, 19, 16, 19,  8,  8, 12, 19,\n",
      "        10, 14,  8,  8,  3, 19, 19, 19,  8, 14, 19,  3, 12, 16,  8, 19, 12,  3,\n",
      "        20, 12,  8, 14, 19, 19, 12,  3, 14,  3, 19, 18, 18,  3,  7,  7,  3, 19,\n",
      "        19,  3, 18, 18, 19, 14, 12,  7,  3, 18, 19,  3, 16,  7,  8, 14, 12, 18,\n",
      "        20, 12,  8,  3, 19, 19, 16, 19,  8, 12,  3, 19,  7, 19, 14,  8,  7, 12,\n",
      "        18, 14,  3, 18, 18, 19, 10, 19, 18,  8, 12,  7, 18, 10, 19,  8,  8,  7,\n",
      "        12, 18, 18,  3, 18, 18, 19, 10, 14, 18, 19, 12,  3, 20, 19,  5, 12, 18,\n",
      "        19,  7,  7, 19,  8, 12, 12, 18, 12,  3, 18, 19, 19, 10,  8,  5,  8, 19,\n",
      "         3, 19,  8,  8, 12, 19, 10, 14,  8,  8,  3, 19, 19, 19,  8, 14,  3, 22,\n",
      "        10,  3,  8,  8, 12,  8, 20,  8, 10,  3, 19, 18,  3, 15,  8, 20, 12, 16,\n",
      "        20,  3, 14, 14,  3, 22,  7,  8,  8, 14, 12,  7, 20,  8,  8,  5, 18, 20,\n",
      "        20, 10,  8, 14, 18, 19, 18, 14, 19, 10,  3, 10, 14, 18,  3, 20, 14, 18,\n",
      "        14, 10, 20, 15, 14,  8, 14, 11, 10, 22,  5, 12,  8, 18, 15, 18,  7,  7,\n",
      "        21, 17, 14,  3, 22, 12,  5, 14, 12, 19, 20, 14, 18, 10, 18,  3,  3, 18,\n",
      "        19, 18, 10,  8,  8, 19, 20, 18, 14, 18,  7, 19, 20, 14, 18,  5, 14,  3,\n",
      "        19,  3,  5, 19,  6,  5, 13, 16, 12, 10,  7,  7, 17,  8, 19, 20, 20, 15,\n",
      "        14,  3, 20, 10,  3, 21, 14,  3,  3, 15, 19,  3, 11, 17,  7,  6,  7, 14,\n",
      "        16, 18, 19,  7,  7, 18, 14, 16, 18,  8, 18, 19, 19, 16, 15, 19, 12,  3,\n",
      "        12, 16,  8, 10,  3,  8, 16, 19,  8, 14, 10,  7, 16, 10, 18, 18, 18, 19,\n",
      "         8, 18, 19, 20,  7,  3, 20,  3, 18,  5,  8, 18, 20, 19, 10,  3, 19, 19,\n",
      "        18, 10, 20, 18, 12, 19, 18,  8])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "sample_image = next(iter(val_dataloader))\n",
    "print(sample_image.keys())\n",
    "print(sample_image['id'][0])\n",
    "print(sample_image['input_ids'][0])\n",
    "print(sample_image['attention_mask'][0])\n",
    "print(sample_image['class_label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class_labels = sample_image['class_label'][0].unsqueeze(0)\n",
    "attention_mask = sample_image['attention_mask'][0].unsqueeze(0)\n",
    "print(class_labels.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "tensor(3.1947, grad_fn=<DivBackward0>)\n",
      "tensor(0.0336, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = sample_image['input_ids'].to(model.device)\n",
    "attention_mask = sample_image['attention_mask'].to(model.device)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "\n",
    "output = model(sample = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                sample_posterior = True, # Should be set to true in training\n",
    ")\n",
    "\n",
    "ce_loss, kl_loss = model.loss_fn(output, input_ids)\n",
    "print(ce_loss)\n",
    "print(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "if config.optimizer == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "elif config.optimizer == \"AdamW\":\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "elif config.optimizer == \"Adamax\":\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "elif config.optimizer == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay, momentum=config.SGDmomentum)\n",
    "else:\n",
    "    raise ValueError(\"Invalid optimizer, choose between `AdamW`, `Adam`, `SGD`, and `Adamax`\")\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs // config.gradient_accumulation_steps),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkj/ProtDiffusion/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "\n",
    "from grokfast import gradfilter_ema\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "@dataclass\n",
    "class TrainingVariables:\n",
    "    global_step: int = 0\n",
    "    val_loss: float = float(\"inf\")\n",
    "    grads: Optional[torch.Tensor] = None\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.__dict__\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.__dict__.update(state_dict)\n",
    "training_variables = TrainingVariables()\n",
    "\n",
    "class VAETrainer:\n",
    "    def __init__(self, \n",
    "                 model: AutoencoderKL1D, \n",
    "                 tokenizer: PreTrainedTokenizerFast, \n",
    "                 optimizer: Union[torch.optim.Adam, torch.optim.AdamW, torch.optim.SGD, torch.optim.Adamax],\n",
    "                 lr_scheduler: LRScheduler, \n",
    "                 train_dataloader: DataLoader, \n",
    "                 val_dataloader: DataLoader, \n",
    "                 config: TrainingConfig, \n",
    "                 training_variables: TrainingVariables = training_variables,\n",
    "                 test_dataloader: DataLoader = None\n",
    "        ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.training_variables = training_variables\n",
    "        self.accelerator_config = ProjectConfiguration(\n",
    "            project_dir=self.config.output_dir,\n",
    "            logging_dir=os.path.join(self.config.output_dir, \"logs\"),\n",
    "            automatic_checkpoint_naming=self.config.automatic_checkpoint_naming,\n",
    "            total_limit=self.config.total_limit, # Limit the total number of checkpoints to 1\n",
    "        )\n",
    "        self.accelerator = Accelerator(\n",
    "            project_config=self.accelerator_config,\n",
    "            mixed_precision=self.config.mixed_precision,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            log_with=\"tensorboard\",\n",
    "        )\n",
    "        # Prepare everything\n",
    "        # There is no specific order to remember, you just need to unpack the\n",
    "        # objects in the same order you gave them to the prepare method.\n",
    "        self.model, self.optimizer, self.train_dataloader, self.test_dataloader, self.val_dataloader, self.lr_scheduler = self.accelerator.prepare(\n",
    "            model, optimizer, train_dataloader, test_dataloader, val_dataloader, lr_scheduler\n",
    "        )\n",
    "        self.accelerator.register_for_checkpointing(self.training_variables)\n",
    "\n",
    "    def logits_to_token_ids(self, logits):\n",
    "        '''\n",
    "        Convert a batch of logits to token_ids.\n",
    "        Returns token_ids\n",
    "        '''\n",
    "        if self.config.cutoff is None:\n",
    "            token_ids = logits.argmax(dim=1)\n",
    "        else:\n",
    "            token_ids = torch.where(logits.max(dim=1).values > self.config.cutoff, \n",
    "                                    logits.argmax(dim=1), \n",
    "                                    torch.tensor([self.tokenizer.unknown_token_id])\n",
    "                                    )\n",
    "        return token_ids\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self\n",
    "    ) -> dict:\n",
    "\n",
    "        test_dir = os.path.join(self.config.output_dir, \"samples\")\n",
    "        os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        num_correct_residues = 0\n",
    "        total_residues = 0\n",
    "        name = f\"step_{self.training_variables.global_step//1000:04d}k\"\n",
    "\n",
    "        progress_bar = tqdm(total=len(self.val_dataloader), disable=not self.accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Evaluating {name}\")\n",
    "\n",
    "        for i, sample in enumerate(self.val_dataloader):\n",
    "\n",
    "            output = self.model(sample = sample['input_ids'],\n",
    "                                attention_mask = sample['attention_mask'],\n",
    "                                sample_posterior = True, # Should be set to False in inference\n",
    "            )\n",
    "\n",
    "            ce_loss, kl_loss = self.model.loss_fn(output, sample['input_ids'])\n",
    "            loss = ce_loss + kl_loss * self.config.kl_weight\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            token_ids_pred = self.logits_to_token_ids(output.sample)\n",
    "\n",
    "            token_ids_correct = ((sample['input_ids'] == token_ids_pred) & (sample['attention_mask'] == 1)).long()\n",
    "            num_residues = torch.sum(sample['attention_mask'], dim=1).long()\n",
    "\n",
    "            num_correct_residues += token_ids_correct.sum().item()\n",
    "            total_residues += num_residues.sum().item()\n",
    "\n",
    "            # Decode the predicted sequences, and remove zero padding\n",
    "            seqs_pred = self.tokenizer.batch_decode(token_ids_pred, skip_special_tokens=self.config.skip_special_tokens)\n",
    "            seqs_lens = torch.sum(sample['attention_mask'], dim=1).long()\n",
    "            seqs_pred = [seq[:i] for seq, i in zip(seqs_pred, seqs_lens)]\n",
    "\n",
    "            # Save all samples as a FASTA file\n",
    "            seq_record_list = [SeqRecord(Seq(seq), id=str(sample['id'][i]), \n",
    "                            description=\n",
    "                            f\"classlabel: {sample['class_label'][i].item()} acc: {token_ids_correct[i].sum().item() / num_residues[i].item():.2f}\")\n",
    "                            for i, seq in enumerate(seqs_pred)]\n",
    "            with open(f\"{test_dir}/{name}.fa\", \"a\") as f:\n",
    "                SeqIO.write(seq_record_list, f, \"fasta\")\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        acc = num_correct_residues / total_residues\n",
    "        print(f\"{name}, val_loss: {running_loss / len(self.val_dataloader):.4f}, val_accuracy: {acc:.4f}\")\n",
    "        logs = {\"val_loss\": loss.detach().item(), \n",
    "                \"val_ce_loss\": ce_loss.detach().item(), \n",
    "                \"val_kl_loss\": kl_loss.detach().item(),\n",
    "                \"val_acc\": acc,\n",
    "                }\n",
    "        return logs\n",
    "    \n",
    "    def train_loop(self, from_checkpoint: Optional[int] = None):\n",
    "  \n",
    "        # start the loop\n",
    "        if self.accelerator.is_main_process:\n",
    "            os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "            os.makedirs(self.accelerator_config.logging_dir, exist_ok=True)\n",
    "            if self.config.push_to_hub:\n",
    "                raise NotImplementedError(\"Pushing to the HF Hub is not implemented yet\")\n",
    "            \n",
    "            if from_checkpoint is not None:\n",
    "                input_dir = os.path.join(self.config.output_dir, \"checkpoints\", f'checkpoint_{from_checkpoint}')\n",
    "                self.accelerator.load_state(input_dir=input_dir)\n",
    "                print(f\"Loaded checkpoint from {input_dir}\")\n",
    "                print(f\"Starting from step {self.training_variables.global_step}\")\n",
    "                print(f\"Validation loss: {self.training_variables.val_loss}\")\n",
    "            else:\n",
    "                self.training_variables.global_step = 0\n",
    "                self.training_variables.val_loss = float(\"inf\")\n",
    "                self.training_variables.grads = None # Initialize the grads for the grokfast algorithm\n",
    "\n",
    "        # Now you train the model\n",
    "        self.model.train()\n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            progress_bar = tqdm(total=len(self.train_dataloader), disable=not self.accelerator.is_local_main_process)\n",
    "            progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            for step, batch in enumerate(self.train_dataloader):\n",
    "\n",
    "                with self.accelerator.accumulate(self.model):\n",
    "                    input = batch['input_ids']\n",
    "                    attention_mask = batch['attention_mask']\n",
    "                    # Predict the noise residual\n",
    "                    output = self.model(sample = input,\n",
    "                                    attention_mask = attention_mask,\n",
    "                                    sample_posterior = True, # Should be set to true in training\n",
    "                    )\n",
    "                    \n",
    "                    ce_loss, kl_loss = self.model.loss_fn(output, input)\n",
    "                    loss = ce_loss + kl_loss * self.config.kl_weight\n",
    "                    self.accelerator.backward(loss)\n",
    "\n",
    "                    if self.config.grokfast:\n",
    "                        self.training_variables.grads = gradfilter_ema(self.model, grads=self.training_variables.grads, alpha=self.config.grokfast_alpha, lamb=self.config.grokfast_lamb) \n",
    "\n",
    "                    self.optimizer.step()\n",
    "                    self.lr_scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                progress_bar.update(1)\n",
    "                logs = {\"train_loss\": loss.detach().item(), \n",
    "                        \"train_ce_loss\": ce_loss.detach().item(), \n",
    "                        \"train_kl_loss\": kl_loss.detach().item(), \n",
    "                        \"lr\": lr_scheduler.get_last_lr()[0], \n",
    "                        \"step\": self.training_variables.global_step,\n",
    "                }\n",
    "                progress_bar.set_postfix(**logs)\n",
    "                self.accelerator.log(logs, step=self.training_variables.global_step)\n",
    "                self.training_variables.global_step += 1\n",
    "\n",
    "                if self.training_variables.global_step == 1 or self.training_variables.global_step % self.config.save_image_model_steps == 0 or self.training_variables.global_step == len(self.train_dataloader):\n",
    "                    self.accelerator.wait_for_everyone()\n",
    "                    self.model.eval() # Set model to eval mode to generate images\n",
    "                    logs = self.evaluate()\n",
    "                    self.accelerator.log(logs, step=self.training_variables.global_step)\n",
    "\n",
    "                    new_val_loss = logs[\"val_loss\"]\n",
    "\n",
    "                    if new_val_loss < self.training_variables.val_loss: # Save the model if the validation loss is lower\n",
    "                        self.training_variables.val_loss = new_val_loss\n",
    "                        self.accelerator.save_state(\n",
    "                            output_dir=self.config.output_dir,\n",
    "                        )\n",
    "                    self.model.train() # Set model back to train mode\n",
    "\n",
    "Trainer = VAETrainer(model, tokenizer, optimizer, lr_scheduler, train_dataloader, val_dataloader, config, training_variables, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(Trainer.train_loop, num_processes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(Trainer.train_loop(from_checkpoint=2), num_processes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
