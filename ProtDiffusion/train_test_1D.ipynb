{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    train_batch_size = 16\n",
    "    mega_batch = 100 # how many batches to use for batchsampling\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    eval_seq_len = 48  # the generated image resolution\n",
    "    num_epochs = 200  # the number of epochs to train the model\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"protein-diffusion\"  # the model name locally and on the HF Hub\n",
    "    pad_to_multiple_of = 16\n",
    "\n",
    "    class_embeddings_concat = False  # whether to concatenate the class embeddings to the time embeddings\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    hub_model_id = \"kkj15dk/AA_test\"  # the name of the repository to create on the HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "    labels_file = 'labels_test.json'\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "config.dataset_name = \"kkj15dk/test_dataset\"\n",
    "dataset = load_dataset(config.dataset_name, download_mode='force_redownload')\n",
    "seed = 42\n",
    "dataset = dataset.shuffle(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"kkj15dk/proteindiffusion_tokenizer\")\n",
    "\n",
    "def encode(example):\n",
    "    return tokenizer(example['sequence'],\n",
    "                    padding = True,\n",
    "                    pad_to_multiple_of = config.pad_to_multiple_of,\n",
    "                    return_token_type_ids=False,\n",
    "                    return_attention_mask=True,\n",
    ")\n",
    "dataset_train = dataset['train'].map(encode, batched=False, remove_columns=[\"sequence\"])\n",
    "dataset_test = dataset['test'].map(encode, batched=False, remove_columns=[\"sequence\"])\n",
    "dataset_val = dataset['val'].map(encode, batched=False, remove_columns=[\"sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train)\n",
    "print(dataset_train[0]['input_ids'])\n",
    "print(dataset_train[0]['class'])\n",
    "print(len(dataset_train[0]['input_ids']))\n",
    "print(tokenizer.decode(dataset_train[0]['input_ids'], skip_special_tokens=False))\n",
    "print(tokenizer.decode(dataset_train[0]['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids_tensor = torch.tensor(dataset_train[0]['input_ids'], dtype=torch.long)\n",
    "onehot_seq = F.one_hot(input_ids_tensor, num_classes=tokenizer.vocab_size + 1).permute(1, 0).unsqueeze(0)\n",
    "print(onehot_seq.shape)\n",
    "\n",
    "def tensor_to_seq(tokenizer, tensor, cutoff = None):\n",
    "        '''\n",
    "        Convert a tensor to a seq using the tokenizer.\n",
    "        '''\n",
    "        \n",
    "        if cutoff is None:\n",
    "            token_ids = tensor.argmax(dim=1)\n",
    "        else:\n",
    "            token_ids = torch.where(tensor.max(dim=1).values > cutoff, \n",
    "                                    tensor.argmax(dim=1), \n",
    "                                    torch.tensor([tokenizer.unknown_token_id])\n",
    "                                    )\n",
    "        print(token_ids)\n",
    "\n",
    "        return tokenizer.batch_decode(token_ids)\n",
    "\n",
    "print(tensor_to_seq(tokenizer, onehot_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch): # Can definitely be optimized\n",
    "    max_len = max(len(x['input_ids']) for x in batch)\n",
    "    input_ids = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    onehot = torch.zeros(len(batch), tokenizer.vocab_size + 1, max_len, dtype=torch.float)\n",
    "    attention_mask = torch.zeros(len(batch), max_len, dtype=torch.float)\n",
    "    class_labels = torch.zeros(len(batch), dtype=torch.long)\n",
    "    for i, x in enumerate(batch):\n",
    "        seq_len = len(x['input_ids'])\n",
    "        input_ids[i, :seq_len] = torch.tensor(x['input_ids'], dtype=torch.long)\n",
    "        onehot_seq = F.one_hot(input_ids[i, :seq_len], num_classes=tokenizer.vocab_size + 1).permute(1, 0)\n",
    "        onehot[i, :, :seq_len] = onehot_seq\n",
    "        attention_mask[i, :seq_len] = torch.tensor(1, dtype=torch.float)\n",
    "        class_labels[i] = torch.tensor(x['class'], dtype=torch.long)\n",
    "    return {'input_ids': input_ids, 'onehot': onehot, 'attention_mask': attention_mask, 'class_label': class_labels}\n",
    "\n",
    "class BatchSampler:\n",
    "    '''\n",
    "    BatchSampler for variable length sequences, batching by similar lengths, to prevent excessive padding.\n",
    "    '''\n",
    "    def __init__(self, lengths, batch_size, mega_batch_size, drop_last = True):\n",
    "        self.lengths = lengths\n",
    "        self.batch_size = batch_size\n",
    "        self.mega_batch_size = mega_batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self):\n",
    "        size = len(self.lengths)\n",
    "        indices = list(range(size))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        step = self.mega_batch_size * self.batch_size\n",
    "        for i in range(0, size, step):\n",
    "            pool = indices[i:i+step]\n",
    "            pool = sorted(pool, key=lambda x: self.lengths[x])\n",
    "            mega_batch_indices = list(range(0, len(pool), self.batch_size))\n",
    "            random.shuffle(mega_batch_indices) # shuffle the mega batches, so that the model doesn't see the same order of lengths every time. The small batch will however always be the one with longest lengths\n",
    "            for j in mega_batch_indices:\n",
    "                if j + self.batch_size > len(pool):  # assume drop_last=True\n",
    "                    if self.drop_last:\n",
    "                        continue\n",
    "                yield pool[j:j+self.batch_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lengths) // self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_lengths = list(map(lambda x: len(x[\"input_ids\"]), dataset_train))\n",
    "test_lengths = list(map(lambda x: len(x[\"input_ids\"]), dataset_test))\n",
    "val_lengths = list(map(lambda x: len(x[\"input_ids\"]), dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, \n",
    "                            batch_sampler=BatchSampler(train_lengths, \n",
    "                                                    config.train_batch_size,\n",
    "                                os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "                os.makedirs(self.accelerator_config.logging_dir, exist_ok=True)                    config.mega_batch,\n",
    "                                                    drop_last=False), \n",
    "                            collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(dataset_test,\n",
    "                            batch_sampler=BatchSampler(test_lengths, \n",
    "                                                    config.eval_batch_size,\n",
    "                                                    config.mega_batch,\n",
    "                                                    drop_last=False), \n",
    "                            collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(dataset_val, \n",
    "                            batch_sampler=BatchSampler(val_lengths, \n",
    "                                                    config.eval_batch_size,\n",
    "                                                    config.mega_batch,\n",
    "                                                    drop_last=False),\n",
    "                            collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from New1D.unet_1d import UNet1DConditionModel\n",
    "\n",
    "model = UNet1DConditionModel(\n",
    "    sample_size=config.eval_seq_len,  # the target image resolution\n",
    "    in_channels=tokenizer.vocab_size + 1,  # the number of input channels,\n",
    "    out_channels=tokenizer.vocab_size + 1,  # the number of output channels\n",
    "    num_class_embeds=2,  # the number of class embeddings\n",
    "    \n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(64, 128, 256, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock1D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock1D\",\n",
    "        \"DownBlock1D\",\n",
    "        \"AttnDownBlock1D\",  # a ResNet downsampling block with spatial self-attention\n",
    "    ),\n",
    "    mid_block_type=\"UNetMidBlock1D\",\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock1D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock1D\",\n",
    "        \"UpBlock1D\",\n",
    "        \"UpBlock1D\",  # a regular ResNet upsampling block\n",
    "    ),\n",
    "    num_attention_heads=8,  # the number of attention heads in the spatial self-attention blocks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.children\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = next(iter(val_dataloader))\n",
    "print(sample_image['input_ids'][0])\n",
    "print(sample_image['attention_mask'][0])\n",
    "print(sample_image['onehot'][0])\n",
    "print(sample_image['class_label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "sample_seq = sample_image['onehot'][0].unsqueeze(0)\n",
    "print(sample_seq.shape)\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "noise = torch.randn(sample_seq.shape)\n",
    "timesteps = torch.LongTensor([50])\n",
    "noisy_seq = noise_scheduler.add_noise(sample_seq, noise, timesteps)\n",
    "\n",
    "class_labels = sample_image['class_label'][0].unsqueeze(0)\n",
    "attention_mask = sample_image['attention_mask'][0].unsqueeze(0)\n",
    "print(class_labels.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "noise_pred = model(sample = noisy_seq, \n",
    "                   timestep = timesteps,\n",
    "                   class_labels = class_labels,\n",
    "                   attention_mask = attention_mask,\n",
    "                   ).sample\n",
    "loss = F.mse_loss(noise_pred, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from New1D.pipeline_protein import DDPMProteinPipeline\n",
    "import os\n",
    "import json\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "\n",
    "    class_labels = torch.randint(0, \n",
    "                                 pipeline.unet.config.num_class_embeds, \n",
    "                                 (config.eval_batch_size,), \n",
    "                                 device=pipeline.device)\n",
    "\n",
    "    seqs = pipeline(\n",
    "        seq_len=config.eval_seq_len,\n",
    "        batch_size=config.eval_batch_size,\n",
    "        class_labels=class_labels, # Sample random class labels\n",
    "        generator=torch.Generator(device='cpu').manual_seed(config.seed), # Use a separate torch generator to avoid rewinding the random state of the main training loop\n",
    "        output_type=\"aa_seq\",\n",
    "        cutoff=None,\n",
    "    ).seqs\n",
    "\n",
    "    with open(config.labels_file, 'r') as f:\n",
    "        labels = json.load(f)\n",
    "    \n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over the dictionary\n",
    "    def getarc(cl, data = labels):\n",
    "        for key, value in data.items():\n",
    "            if value['class'] == cl:\n",
    "                return value['architecture']\n",
    "\n",
    "    # Save all samples as a FASTA file\n",
    "    seq_record_list = [SeqRecord(Seq(seq), id=str(i), \n",
    "                    description=\"classlabel: \" + str(class_labels[i].item()) + \n",
    "                                \" w: \" + str('N/A') + \n",
    "                                \" arc: \" + str(getarc(class_labels[i]))) for i, seq in enumerate(seqs)]\n",
    "    with open(f\"{test_dir}/{epoch:04d}.fa\", \"w\") as f:\n",
    "        SeqIO.write(seq_record_list, f, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        if config.push_to_hub:\n",
    "            repo_id = create_repo(\n",
    "                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n",
    "            ).repo_id\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the\n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_seqs = batch[\"onehot\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            class_labels = batch[\"class_label\"]\n",
    "            \n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_seqs.shape, device=clean_seqs.device)\n",
    "            bs = clean_seqs.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_seqs.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_seq = noise_scheduler.add_noise(clean_seqs, noise, timesteps)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(sample = noisy_seq, \n",
    "                                    timestep = timesteps,\n",
    "                                    class_labels = class_labels,\n",
    "                                    attention_mask = attention_mask,\n",
    "                                    return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMProteinPipeline(unet=accelerator.unwrap_model(model), \n",
    "                                           scheduler=noise_scheduler, \n",
    "                                           tokenizer=tokenizer,\n",
    "                                           ).to(accelerator.device)\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                if config.push_to_hub:\n",
    "                    upload_folder(\n",
    "                        repo_id=repo_id,\n",
    "                        folder_path=config.output_dir,\n",
    "                        commit_message=f\"Epoch {epoch}\",\n",
    "                        ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "                    )\n",
    "                else:\n",
    "                    pipeline.save_pretrained(config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "sample_images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\n",
    "Image.open(sample_images[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
