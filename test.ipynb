{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tensor = torch.tensor([[1,1,1,0,1,1,1,1,0,0],[1,1,1,0,1,1,0,0,0,0]], dtype=torch.int64)\n",
    "print(tensor)\n",
    "\n",
    "tensor = -tensor.to(torch.float32)\n",
    "tensor = F.max_pool1d(tensor, kernel_size=2, stride=2, padding=0)\n",
    "tensor = -tensor.to(torch.int64)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[1,0,0,1,0,0],[1,0,1,0,0,1]], dtype=torch.int64)\n",
    "print(tensor)\n",
    "\n",
    "tensor = tensor.to(torch.float32).unsqueeze(1)\n",
    "tensor = F.interpolate(tensor, scale_factor=2, mode=\"nearest\")\n",
    "tensor = tensor.to(torch.int64).squeeze(1)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('/home/kkj/ProtDiffusion/ProtDiffusion/tokenizer/tokenizer_v4.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer('-[ACDFGDIGDE]---',\n",
    "                        padding=True,\n",
    "                        truncation=False, # We truncate the sequences beforehand\n",
    "                        return_token_type_ids=False,\n",
    "                        return_attention_mask=True, # We need to attend to padding tokens, so we set this to False\n",
    "                        return_tensors=\"pt\",\n",
    ")\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized['attention_mask'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.randint(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def round_length(length: int, pad: int = 2, rounding: int = 16) -> int:\n",
    "    '''\n",
    "    Round the length to the nearest multiple of 16.\n",
    "    '''\n",
    "    return int(np.ceil((length + pad) / rounding) * rounding)\n",
    "\n",
    "def process_sequence(sequence: str,\n",
    "                     bos_token: str = \"[\",\n",
    "                     eos_token: str = \"]\",\n",
    "                     pad_token: str = \"-\",\n",
    ") -> str:\n",
    "    '''\n",
    "    Process the sequence by adding the bos and eos tokens, and padding it to a multiple of 16 (or what the variable is set to in the round_kength).\n",
    "    Return the sequence and the length of the sequence.\n",
    "    '''\n",
    "    seq_len = round_length(len(sequence))\n",
    "    sequence = bos_token + sequence + eos_token\n",
    "    len_diff = seq_len - len(sequence)\n",
    "    rand_int = random.randint(0, len_diff)\n",
    "    sequence = pad_token * rand_int + sequence + pad_token * (len_diff - rand_int)\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sequence('ACDFGDIGDEIGH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProtDiffusion.models.dit_transformer_1d import DiTTransformer1DModel\n",
    "from ProtDiffusion.training_utils import count_parameters\n",
    "\n",
    "model = DiTTransformer1DModel(\n",
    "    num_attention_heads = 8,\n",
    "    attention_head_dim = 72,\n",
    "    in_channels = 64,\n",
    "    num_layers = 8,\n",
    "    attention_bias = True,\n",
    "    activation_fn = \"gelu-approximate\",\n",
    "    num_classes = 2,\n",
    "    upcast_attention = False,\n",
    "    norm_type = \"ada_norm_zero\",\n",
    "    norm_elementwise_affine = False,\n",
    "    norm_eps = 1e-5,\n",
    "    pos_embed_type = \"sinusoidal\", # sinusoidal\n",
    "    num_positional_embeddings = 1024,\n",
    "    use_rope_embed = True, # RoPE https://github.com/lucidrains/rotary-embedding-torch\n",
    ").to('cuda')\n",
    "count_parameters(model)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(16, 64, 1008).to('cuda') # Batch size, in channels, sequence length, max length is 1024\n",
    "m = torch.randint(0, 2, (16, 1008), dtype=torch.bool).to('cuda')\n",
    "t = torch.randint(0, 1000, (16,), dtype=torch.int64).to('cuda') # Timesteps, any int is valid?\n",
    "cl = torch.randint(0, 2, (16,), dtype=torch.int64).to('cuda') # Classifier labels, 0 and 1 are the only valid labels, 2 is a dropped label\n",
    "\n",
    "out = model(x, m, t, cl)\n",
    "print(out.sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProtDiffusion.models.pipeline_protein import ProtDiffusionPipeline\n",
    "from ProtDiffusion.models.autoencoder_kl_1d import AutoencoderKL1D\n",
    "from diffusers.schedulers import DDPMScheduler\n",
    "\n",
    "vae_ce = AutoencoderKL1D.from_pretrained('/home/kkj/ProtDiffusion/output/protein-VAE-UniRef50_v18.1/pretrained/CE')\n",
    "vae_ema = AutoencoderKL1D.from_pretrained('/home/kkj/ProtDiffusion/output/protein-VAE-UniRef50_v18.1/pretrained/EMA')\n",
    "\n",
    "scheduler = DDPMScheduler()\n",
    "\n",
    "pipeline = ProtDiffusionPipeline(\n",
    "    transformer=model,\n",
    "    vae=vae_ema,\n",
    "    scheduler=scheduler,\n",
    "    tokenizer=tokenizer,\n",
    ").to('cuda')\n",
    "\n",
    "out1 = pipeline(seq_len=[64,64], \n",
    "               class_labels=[0,0], \n",
    "               guidance_scale=4.0,\n",
    "               num_inference_steps=2,\n",
    "               generator=torch.Generator().manual_seed(42),\n",
    "               output_type='aa_seq',\n",
    ").seqs\n",
    "out2 = pipeline(seq_len=[64,256], \n",
    "               class_labels=[0,0], \n",
    "               guidance_scale=4.0,\n",
    "               num_inference_steps=10,\n",
    "               generator=torch.Generator().manual_seed(42),\n",
    "               output_type='aa_seq',\n",
    ").seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in out1:\n",
    "    print(seq)\n",
    "for seq in out2:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = torch.randn(2, 64, 8).to('cuda')\n",
    "attention_mask = torch.tensor([[1,1,1,1,1,1,1,1],[1,1,1,1,0,0,0,0]]).to('cuda')\n",
    "latents = latents * attention_mask.unsqueeze(1)\n",
    "print(latents[:,0,:])\n",
    "vae_ema = vae_ema.to('cuda')\n",
    "decoded = vae_ema.decode(latents, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProtDiffusion.training_utils import logits_to_token_ids\n",
    "\n",
    "token_ids = logits_to_token_ids(decoded.sample, tokenizer)\n",
    "output = tokenizer.batch_decode(token_ids)\n",
    "seqs = output\n",
    "for seq in seqs:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "one = torch.randn(2, 64, 8)\n",
    "two = torch.tensor([[1,1,1,1,1,1,1,1],[1,1,1,1,1,1,0,0]], dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([896])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two = two.unsqueeze(1).expand_as(one)\n",
    "one[two].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
