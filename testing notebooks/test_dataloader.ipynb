{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from ProtDiffusion.training_utils import VAETrainingConfig, make_clustered_dataloader, set_seed, VAETrainer, count_parameters\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from ProtDiffusion.models.autoencoder_kl_1d import AutoencoderKL1D\n",
    "\n",
    "import os\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed) # Set the random seed for reproducibility\n",
    "\n",
    "dataset = load_from_disk('/home/kkj/ProtDiffusion/datasets/UniRef50_grouped')\n",
    "# dataset = load_from_disk('/home/kkj/ProtDiffusion/datasets/ACP-bad?_grouped')\n",
    "dataset = dataset.shuffle(seed)\n",
    "\n",
    "# %%\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/zhome/fb/0/155603/ProtDiffusion/ProtDiffusion/tokenizer/tokenizer_v4.1\")\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/home/kkj/ProtDiffusion/ProtDiffusion/tokenizer/tokenizer_v4.1\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/home/kkj/ProtDiffusion/ProtDiffusion/tokenizer/tokenizer_v4.2\")\n",
    "\n",
    "# Split the dataset into train and temp sets using the datasets library\n",
    "train_test_split_ratio = 0.0002\n",
    "train_val_test_split = dataset.train_test_split(test_size=train_test_split_ratio, seed=seed)\n",
    "train_dataset = train_val_test_split['train']\n",
    "temp_dataset = train_val_test_split['test']\n",
    "\n",
    "# Split the temp set into validation and test sets using the datasets library\n",
    "val_test_split_ratio = 0.5\n",
    "val_test_split = temp_dataset.train_test_split(test_size=val_test_split_ratio, seed=seed)\n",
    "val_dataset = val_test_split['train']\n",
    "test_dataset = val_test_split['test']\n",
    "\n",
    "# Check dataset lengths\n",
    "print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset length: {len(val_dataset)}\")\n",
    "print(f\"Test dataset length: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_batch = 100\n",
    "batch_size = 16\n",
    "max_len_start = 64\n",
    "max_len = max_len_start\n",
    "\n",
    "print(\"num cpu cores:\", os.cpu_count())\n",
    "print(\"setting num_workers to 16\")\n",
    "num_workers = 16\n",
    "\n",
    "train_dataloader = make_clustered_dataloader(batch_size,\n",
    "                                             mega_batch,\n",
    "                                             train_dataset,\n",
    "                                             tokenizer=tokenizer,\n",
    "                                             max_len=max_len_start,\n",
    "                                             num_workers=num_workers,\n",
    ")\n",
    "val_dataloader = make_clustered_dataloader(batch_size,\n",
    "                                           mega_batch,\n",
    "                                           val_dataset, \n",
    "                                           tokenizer=tokenizer,\n",
    "                                           max_len=max_len, \n",
    "                                           num_workers=1,\n",
    ")\n",
    "test_dataloader = make_clustered_dataloader(batch_size,\n",
    "                                            mega_batch,\n",
    "                                            test_dataset,\n",
    "                                            tokenizer=tokenizer,\n",
    "                                            max_len=max_len, \n",
    "                                            num_workers=1,\n",
    ")\n",
    "print(\"length of train dataloader: \", len(train_dataloader))\n",
    "print(\"length of val dataloader: \", len(val_dataloader))\n",
    "print(\"length of test dataloader: \", len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,batch in enumerate(train_dataloader):\n",
    "    print(batch)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "        print(\"batch\", (epoch*len(train_dataloader)) + i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
